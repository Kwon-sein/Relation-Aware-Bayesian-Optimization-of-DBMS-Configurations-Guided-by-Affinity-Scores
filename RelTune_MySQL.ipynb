{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RelTune : Relation-Aware Bayesian Optimization of DBMS Configurations Guided by Affinity Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational Graph Construction\n",
    "- Parameter Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "\n",
    "import json\n",
    "\n",
    "file_path = \"/Description_Embeding_fixed.json\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "param_names = list(data.keys())\n",
    "descriptions = list(data.values())\n",
    "embeddings = model.encode(descriptions, convert_to_tensor=True)\n",
    "desc_embeddings = model.encode(descriptions, convert_to_tensor=True)\n",
    "\n",
    "results = []\n",
    "for i, j in itertools.combinations(range(len(param_names)), 2):\n",
    "    param_i = param_names[i]\n",
    "    param_j = param_names[j]\n",
    "    sim = F.cosine_similarity(embeddings[i].unsqueeze(0), embeddings[j].unsqueeze(0)).item()\n",
    "    results.append((param_i, param_j, sim))\n",
    "    \n",
    "results.sort(key=lambda x: -x[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "edge_list = []\n",
    "similarity_threshold = 0.75\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(i + 1, len(embeddings)):\n",
    "        sim = F.cosine_similarity(embeddings[i], embeddings[j], dim=0).item()\n",
    "        if sim >= similarity_threshold:\n",
    "            edge_list.append((i, j))\n",
    "            edge_list.append((j, i))  \n",
    "\n",
    "\n",
    "edge_index = torch.tensor(edge_list, dtype=torch.long).T  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "x = embeddings  \n",
    "graph_data = Data(x=x, edge_index=edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "\n",
    "G = to_networkx(graph_data, to_undirected=True)\n",
    "\n",
    "\n",
    "param_name_dict = {i: name for i, name in enumerate(param_names)}\n",
    "G = nx.relabel_nodes(G, param_name_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "pos = nx.spring_layout(G, seed=42, k=0.5)  \n",
    "\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_color='skyblue', node_size=500)\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.3)\n",
    "nx.draw_networkx_labels(G, pos, font_size=9)\n",
    "\n",
    "plt.title(\"MySQL Parameter Graph (Cosine ≥ 0.75)\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "config_file = \"configs.csv\"\n",
    "config = pd.read_csv(config_file)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_config = scaler.fit_transform(config)  # shape: [5000, 138]\n",
    "\n",
    "metrics_df = pd.read_csv(\"metrics.csv\")\n",
    "metric_scaler = MinMaxScaler()\n",
    "scaled_metrics = metric_scaler.fit_transform(metrics_df[['tps', 'latency']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dataset = []\n",
    "for i in range(len(scaled_config)):\n",
    "    feature_list = []\n",
    "    for j, param in enumerate(param_names):\n",
    "        value_tensor = torch.tensor([scaled_config[i][j]], dtype=torch.float)  \n",
    "        desc_tensor = desc_embeddings[j]  \n",
    "        node_feature = torch.cat([value_tensor, desc_tensor], dim=0)  \n",
    "        feature_list.append(node_feature)\n",
    "    x = torch.stack(feature_list, dim=0)  \n",
    "    y = torch.tensor(scaled_metrics[i], dtype=torch.float).view(1,-1)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "    graph_dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "metrics = pd.read_csv(\"external_metrics.csv\")\n",
    "\n",
    "metrics = metrics.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "metrics = metrics.replace([np.inf],9999999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knob_list = glob.glob(\"configs/my_*.cnf\")\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for xx in range(len(knob_list)):\n",
    "    path = \"configs/my_{}.cnf\".format(xx)\n",
    "    # knob_list = glob.glob(\"/home/sein/2023_EDBT/KCC_tpcc_dataset/my_*.cnf\")\n",
    "    a_all = pd.read_csv(path, sep=\"=\", names=['Sample', 'value'], header=2)\n",
    "    a_all = a_all.set_index(\"Sample\")\n",
    "    cur_all_df = a_all.T\n",
    "    \n",
    "    if cnt == 0:\n",
    "        configs = cur_all_df\n",
    "    else :\n",
    "        configs = pd.concat([configs, cur_all_df], axis=0)\n",
    "    cnt += 1\n",
    "configs = configs.reset_index()\n",
    "configs = configs.drop([\"index\"],axis=1)\n",
    "configs = configs.drop(configs.columns[[0,1]], axis=1)\n",
    "\n",
    "\n",
    "configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Based Latent Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "\n",
    "class GATEncoder(nn.Module):\n",
    "    def __init__(self, in_dim=385, hidden_dim=128, hidden_sec_dim=64, z_dim=32, heads=4):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATConv(in_dim, hidden_dim // heads, heads=heads)\n",
    "        self.gat2 = GATConv(hidden_dim, hidden_sec_dim // heads, heads=heads)\n",
    "        self.gat3 = GATConv(hidden_sec_dim, z_dim, heads=1, concat=False)  # 마지막은 concat=False로 z_dim 유지\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.elu(self.gat1(x, edge_index))\n",
    "        x = F.elu(self.gat2(x, edge_index))\n",
    "        x = self.gat3(x, edge_index)\n",
    "        z = global_mean_pool(x, batch)\n",
    "        return z  # [batch_size, z_dim]\n",
    "\n",
    "class ConfigDecoder(nn.Module):\n",
    "    def __init__(self, z_dim=32, output_dim=138):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(z_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.fc(z)\n",
    "\n",
    "class Surrogate(nn.Module):\n",
    "    def __init__(self, z_dim=32):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(z_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.fc(z)\n",
    "\n",
    "class GNN_Autoencoder_With_Surrogate(nn.Module):\n",
    "    def __init__(self, in_dim=385, hidden_dim=128, hidden_sec_dim=64, z_dim=32, out_config_dim=138, heads=4):\n",
    "        super().__init__()\n",
    "        self.encoder = GATEncoder(in_dim, hidden_dim, hidden_sec_dim, z_dim, heads=heads)\n",
    "        self.decoder = ConfigDecoder(z_dim, out_config_dim)\n",
    "        self.surrogate = Surrogate(z_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        z = self.encoder(data.x, data.edge_index, data.batch)\n",
    "        recon_config = self.decoder(z)\n",
    "        metric_pred = self.surrogate(z)\n",
    "        return recon_config, metric_pred, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GNN_Autoencoder_With_Surrogate().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "loss_fn = nn.MSELoss()\n",
    "metric_weight = 0.3  \n",
    "\n",
    "train_data, temp_data = train_test_split(graph_dataset, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.3, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64)\n",
    "test_loader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 50\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(1, 3001):\n",
    "    model.train()\n",
    "    total_recon_loss, total_metric_loss, total_combined_loss = 0, 0, 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        recon_config, metric_pred, _ = model(batch)\n",
    "\n",
    "        target_config = batch.x[:, 0].view(batch.num_graphs, -1)          \n",
    "        metric_target = batch.y.view(batch.num_graphs, -1)                \n",
    "\n",
    "        recon_loss = loss_fn(recon_config, target_config)\n",
    "        metric_loss = loss_fn(metric_pred, metric_target)\n",
    "        loss = recon_loss + metric_weight * metric_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_metric_loss += metric_loss.item()\n",
    "        total_combined_loss += loss.item()\n",
    "\n",
    "    avg_recon_loss = total_recon_loss / len(train_loader)\n",
    "    avg_metric_loss = total_metric_loss / len(train_loader)\n",
    "    avg_train_loss = total_combined_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_recon_loss, val_metric_loss, val_combined_loss = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            recon_config, metric_pred, _ = model(batch)\n",
    "\n",
    "            target_config = batch.x[:, 0].view(batch.num_graphs, -1)\n",
    "            metric_target = batch.y.view(batch.num_graphs, -1)\n",
    "\n",
    "            recon_loss = loss_fn(recon_config, target_config)\n",
    "            metric_loss = loss_fn(metric_pred, metric_target)\n",
    "            loss = recon_loss + metric_weight * metric_loss\n",
    "\n",
    "            val_recon_loss += recon_loss.item()\n",
    "            val_metric_loss += metric_loss.item()\n",
    "            val_combined_loss += loss.item()\n",
    "\n",
    "    avg_val_recon = val_recon_loss / len(val_loader)\n",
    "    avg_val_metric = val_metric_loss / len(val_loader)\n",
    "    avg_val_loss = val_combined_loss / len(val_loader)\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"[Epoch {epoch}]\")\n",
    "        print(f\"  Train Recon Loss:  {avg_recon_loss:.4f} | Metric Loss: {avg_metric_loss:.4f} | Total: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val   Recon Loss:  {avg_val_recon:.4f} | Metric Loss: {avg_val_metric:.4f} | Total: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, temp_data = train_test_split(graph_dataset, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.3, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64)\n",
    "test_loader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Space Optimization via Hybrid-Score-Guided Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "\n",
    "aff_score_list = []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "z_dim = 'hyper-parameter'\n",
    "alpha = 'hyper-parameter'   \n",
    "gamma = 'hyper-parameter'   \n",
    "sigma = 'hyper-parameter'   \n",
    "\n",
    "\n",
    "z_list = []\n",
    "y_list = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        _, _, z = model(batch)\n",
    "        z_list.append(z.cpu())\n",
    "        y_list.append(batch.y.cpu())\n",
    "\n",
    "Z_all = torch.cat(z_list, dim=0).numpy()  \n",
    "Y_all = torch.cat(y_list, dim=0).numpy()  \n",
    "\n",
    "\n",
    "def is_good(metric, tps_threshold='hyper-parameter', latency_threshold='hyper-parameter'):\n",
    "\n",
    "    return metric[0] >= tps_threshold and metric[1] <= latency_threshold\n",
    "\n",
    "Z_good = np.array([z for z, m in zip(Z_all, Y_all) if is_good(m)])\n",
    "\n",
    "\n",
    "def affinity_score(z, Z_good, sigma=1.0):\n",
    "    if len(Z_good) == 0:\n",
    "        return 0.0\n",
    "    dists = np.linalg.norm(Z_good - z, axis=1)\n",
    "    weights = np.exp(-dists**2 / (2 * sigma**2))\n",
    "    return np.mean(weights)\n",
    "\n",
    "\n",
    "z_min = Z_all.min(axis=0)\n",
    "z_max = Z_all.max(axis=0)\n",
    "margin = 'hyper-parameter'\n",
    "z_range = z_max - z_min\n",
    "z_min -= margin * z_range\n",
    "z_max += margin * z_range\n",
    "\n",
    "space = [Real(float(z_min[i]), float(z_max[i]), name=f'z_{i}') for i in range(z_dim)]\n",
    "\n",
    "def objective(z_flat):\n",
    "    z_tensor = torch.tensor(z_flat, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        metric = model.surrogate(z_tensor)\n",
    "        tps = metric[:, 0].item()\n",
    "        latency = metric[:, 1].item()\n",
    "        score = tps - alpha * latency\n",
    "\n",
    "    aff = affinity_score(np.array(z_flat), Z_good, sigma=sigma)\n",
    "    hybrid_score = score + gamma * aff\n",
    "\n",
    "    ### 그림그리기 용\n",
    "    aff_score_list.append(hybrid_score)\n",
    "    \n",
    "    return -hybrid_score  \n",
    "\n",
    "res = gp_minimize(\n",
    "    func=objective,\n",
    "    dimensions=space,\n",
    "    n_calls= 300,\n",
    "    n_initial_points=10,\n",
    "    acq_func=\"EI\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "aff_z_trajectory = np.array(res.x_iters)  \n",
    "np.save(\"z_trajectory_hybrid.npy\", aff_z_trajectory)\n",
    "\n",
    "best_z = torch.tensor(res.x, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    best_metric = model.surrogate(best_z)\n",
    "    best_config = model.decoder(best_z)\n",
    "\n",
    "print(\"\\nHybrid Score)\")\n",
    "print(f\"Best Score (TPS - α·Latency + γ·Affinity): {-res.fun:.4f}\")\n",
    "print(f\"TPS: {best_metric[0,0].item():.4f}, Latency: {best_metric[0,1].item():.4f}\")\n",
    "print(\"Recovered Config (normalized):\")\n",
    "print(best_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "knob_list = glob.glob(\"configs/my_*.cnf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "\n",
    "for xx in range(len(knob_list)):\n",
    "    path = \"configs/my_{}.cnf\".format(xx)\n",
    "    a_all = pd.read_csv(path, sep=\"=\", names=['Sample', 'value'], header=2)\n",
    "    a_all = a_all.set_index(\"Sample\")\n",
    "    cur_all_df = a_all.T\n",
    "    \n",
    "    if cnt == 0:\n",
    "        A_config = cur_all_df\n",
    "    else :\n",
    "        A_config = pd.concat([A_config, cur_all_df], axis=0)\n",
    "    cnt += 1\n",
    "A_config = A_config.reset_index()\n",
    "A_config = A_config.drop([\"index\"],axis=1)\n",
    "A_config = A_config.drop(A_config.columns[[0,1]], axis=1)\n",
    "\n",
    "\n",
    "A_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recovered = best_config.cpu().detach().numpy()  \n",
    "original_config = scaler.inverse_transform(recovered.reshape(1, -1)).flatten()\n",
    "\n",
    "for name, value in zip(param_names, original_config):\n",
    "        print(f\"{name} = {int(round(value))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "\n",
    "vbo_score_list = []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "z_dim = 'hyper-parameter'\n",
    "alpha = 'hyper-parameter'\n",
    "\n",
    "z_list, y_list = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        _, _, z = model(batch)\n",
    "        z_list.append(z.cpu())\n",
    "        y_list.append(batch.y.cpu())\n",
    "Z_all = torch.cat(z_list, dim=0).numpy()\n",
    "Y_all = torch.cat(y_list, dim=0).numpy()\n",
    "\n",
    "# 2. 탐색 공간 정의\n",
    "z_min = Z_all.min(axis=0)\n",
    "z_max = Z_all.max(axis=0)\n",
    "margin = 'hyper-parameter'\n",
    "z_min -= margin * (z_max - z_min)\n",
    "z_max += margin * (z_max - z_min)\n",
    "space = [Real(float(z_min[i]), float(z_max[i]), name=f'z_{i}') for i in range(z_dim)]\n",
    "\n",
    "def vanilla_objective(z_flat):\n",
    "    z_tensor = torch.tensor(z_flat, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        metric = model.surrogate(z_tensor)\n",
    "        tps = metric[:, 0].item()\n",
    "        latency = metric[:, 1].item()\n",
    "        score = tps - alpha * latency\n",
    "    vbo_score_list.append(score)       \n",
    "    return -score \n",
    "\n",
    "res = gp_minimize(\n",
    "    func=vanilla_objective,\n",
    "    dimensions=space,\n",
    "    n_calls=300,\n",
    "    n_initial_points=10,\n",
    "    acq_func=\"EI\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "vbo_z_trajectory = np.array(res.x_iters)\n",
    "\n",
    "\n",
    "best_z = torch.tensor(res.x, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    best_metric = model.surrogate(best_z)\n",
    "    best_config = model.decoder(best_z)\n",
    "\n",
    "print(\"[Vanilla BO\")\n",
    "print(f\"Best Score (TPS - α·Latency): {-res.fun:.4f}\")\n",
    "print(f\"TPS: {best_metric[0,0].item():.4f}, Latency: {best_metric[0,1].item():.4f}\")\n",
    "print(\"Recovered Config (normalized):\")\n",
    "print(best_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
